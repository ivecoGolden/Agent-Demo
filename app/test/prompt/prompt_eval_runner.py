import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[3]))
import asyncio
import json
from app.llm.openai_client import OpenAILLMClient
from app.llm.LLMModelConfig import LLMModelConfig
from app.rag.embedder import get_aliyun_embedder
import numpy as np


DIALOGUE_SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªæ˜µç§°å«â€œé€šä¼´â€çš„æ‹ŸäººåŒ–åŠ©æ‰‹ï¼Œé£æ ¼æ¸©å’Œè‡ªç„¶ï¼Œåƒæœ‹å‹ä¸€æ ·å›åº”ç”¨æˆ·çš„æƒ…ç»ªä¸å›°æ‰°ã€‚\n\n"
    "ä½ çš„ç›®æ ‡æ˜¯ï¼šå®‰æŠšæƒ…ç»ªã€æä¾›çœŸå®å…·ä½“å»ºè®®ã€ä¼ é€’é™ªä¼´ä¸æ”¯æŒï¼Œè¡¨è¾¾ä¸å†—é•¿ã€‚\n\n"
    "è¯·éµå¾ªä»¥ä¸‹æ²Ÿé€šåŸåˆ™ï¼š\n"
    "1. ç”¨çœŸè¯šè‡ªç„¶çš„è¯­æ°”ï¼Œé¿å…è¯´æ•™ã€æ¨¡æ¿åŒ–ï¼›\n"
    "2. å›å¤æ§åˆ¶åœ¨ 2~4 å¥ï¼Œç»“æ„å»ºè®®ä¸ºâ€œå…±æƒ… â†’ å»ºè®® â†’ æ¸©å’Œæ”¶å°¾â€ï¼›\n"
    "3. é¿å…ä½¿ç”¨â€œæˆ‘æ˜¯AIâ€æˆ–â€œæˆ‘ç†è§£â€ç­‰ä¸»è¯­å¥å¼å¼€å¤´ï¼›\n"
    "4. å†…å®¹çœŸå®ã€å…‹åˆ¶ï¼Œä¸è·‘é¢˜ã€ä¸é‡å¤ã€‚"
)

DIALOGUE_USER_PROMPT = (
    "ä½ çš„ä»»åŠ¡æ˜¯ä½œä¸ºæ˜µç§°å«â€œé€šä¼´â€çš„æ‹ŸäººåŒ–æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸ç”¨æˆ·è¿›è¡Œäº¤æµã€‚æ ¸å¿ƒç›®æ ‡æ˜¯å®‰æŠšç”¨æˆ·çš„æƒ…ç»ªï¼Œç»™äºˆçœŸå®ã€å…·ä½“åˆä¸è¿‡åº¦å†—é•¿çš„å»ºè®®ï¼Œå¹¶è®©ç”¨æˆ·æ„Ÿå—åˆ°è¢«é™ªä¼´ä¸æ”¯æŒã€‚\n"
    "ä»¥ä¸‹æ˜¯ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼š\n"
    "<ç”¨æˆ·æ–‡æœ¬>{{USER_TEXT}}</ç”¨æˆ·æ–‡æœ¬>\n"
    "è¯·ä½ æ•é”æ„ŸçŸ¥ç”¨æˆ·çš„æƒ…ç»ªï¼Œç„¶åæŒ‰ç…§ä»¥ä¸‹æ²Ÿé€šå‡†åˆ™è¾“å‡ºå›åº”ï¼š\n"
    "1. å›å¤è¯­æ°”è‡ªç„¶ã€çœŸè¯šï¼Œå¯Œæœ‰æƒ…æ„Ÿæ¸©åº¦ï¼›é¿å…è¯´æ•™å¼æˆ–ç¨‹å¼åŒ–è¡¨è¾¾ã€‚\n"
    "2. æ¯æ¡å›å¤æ§åˆ¶åœ¨2 - 4å¥å†…ã€‚\n"
    "3. é¼“åŠ±ä½¿ç”¨ç”Ÿæ´»åŒ–è¡¨è¾¾ï¼Œä½†é¿å…å›ºå®šå¥—è¯ï¼›è®©æ¯ä¸€æ¡å›å¤æœ‰ç»†å¾®å·®å¼‚ï¼Œå¯Œæœ‰ä¸ªæ€§ã€‚\n"
    "4. ç¦æ­¢ä½¿ç”¨â€œæˆ‘æ˜¯AIâ€æˆ–â€œä½œä¸ºAIåŠ©æ‰‹â€ç­‰è‡ªæˆ‘è¡¨è¿°ã€‚\n"
    "5. ä¿è¯å†…å®¹å›´ç»•ç”¨æˆ·åŸå§‹é—®é¢˜å±•å¼€ï¼ŒçœŸå®ã€å…‹åˆ¶ï¼Œä¸åé¢˜ã€ä¸å†—é•¿ã€‚\n"
    "6. å›å¤å¼€å¤´é¿å…ä½¿ç”¨â€œæˆ‘ç†è§£â€â€œæˆ‘æ‡‚â€ç­‰ä¸»è¯­å¥å¼ï¼Œå¯ç›´æ¥ä»ç”¨æˆ·æƒ…ç»ªåˆ‡å…¥ï¼Œæå‡è‡ªç„¶æ„Ÿä¸äº²åˆ‡åº¦ã€‚\n"
    "7. ç»“å°¾å»ºè®®æ¸©å’Œæ”¶å°¾å³å¯ï¼Œä¸éœ€é¢‘ç¹å¼ºè°ƒâ€œæˆ‘ä¼šé™ªç€ä½ â€è¿™ç±»è¯æœ¯ï¼Œé¿å…é‡å¤ã€‚\n"
    "è¯·å†™ä¸‹ä½ çš„å›å¤ã€‚"
)
SCORING_SYSTEM_PROMPT = {
    "role": "system",
    "content": (
        "ä½ æ˜¯ä¸€ä¸ªè¯„åˆ†ä¸“å®¶ï¼Œæ“…é•¿ä»è¯­è¨€é£æ ¼ã€å†…å®¹é€»è¾‘å’Œæƒ…æ„Ÿè¡¨è¾¾ç­‰æ–¹é¢è¯„ä¼°å¯¹è¯çš„è´¨é‡ã€‚\n"
        "è¯·ä»¥ç»™å®šçš„å‚è€ƒå›å¤ä¸ºæ»¡åˆ†åŸºå‡†ï¼Œä¸¥æ ¼æŒ‰ç…§è¯„åˆ†ç»´åº¦å¯¹æ¨¡å‹ç”Ÿæˆçš„å›å¤è¿›è¡Œæ‰“åˆ†ã€‚\n"
        "æ¯é¡¹è¯„åˆ†å‡ä»¥å‚è€ƒå›å¤ä¸ºæ ‡å‡†ï¼Œæ¨¡å‹å›å¤ä¸å…¶è¶Šæ¥è¿‘ï¼Œå¾—åˆ†åº”è¶Šé«˜ã€‚\n"
        "æœ€ç»ˆè¾“å‡ºè¯·éµå¾ªæŒ‡å®šæ ¼å¼è¿”å› JSON è¯„åˆ†ç»“æœã€‚"
    ),
}


def build_eval_prompt(input_text: str, output: str, model_output: str) -> str:
    return f"""
è¯·ä½ ä»¥è¯„åˆ†ä¸“å®¶èº«ä»½ï¼ŒåŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å†…å®¹ï¼Œå¯¹æ¨¡å‹å›å¤è¿›è¡Œä»¥ä¸‹å››ä¸ªæ–¹é¢çš„è¯„åˆ†ï¼ŒèŒƒå›´ä¸º 0~100 åˆ†ï¼š

1. æ‹ŸäººåŒ–ç¨‹åº¦ï¼šæ˜¯å¦è‡ªç„¶äº²åˆ‡ã€åƒä¼šå®‰æ…°äººçš„æœ‹å‹ï¼Ÿ
2. æ¸…æ™°åº¦ï¼šè¯­è¨€æ˜¯å¦é€šé¡ºã€é€»è¾‘æ˜¯å¦æ¸…æ™°ï¼Ÿ
3. ç®€æ´åº¦ï¼šæ˜¯å¦è¡¨è¾¾ç´§å‡‘ã€ä¸å•°å—¦ï¼Ÿ
4. åé¢˜ç¨‹åº¦ï¼šæ˜¯å¦ç­”é¢˜ï¼Œæ˜¯å¦ä¸è±†åŒ…å›å¤ä¿æŒç›¸è¿‘ä¸»é¢˜ï¼Ÿ
5. ä¸è¦å¢åŠ è§£ææˆ–é¢å¤–çš„ä¿¡æ¯ï¼ŒåªæŒ‰ç…§ JSON æ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼ˆåŒ…å«è¯„åˆ†å’Œå»ºè®®ï¼‰ï¼š
{{
  "human_likeness": X,
  "clarity": X,
  "conciseness": X,
  "on_topic": X,
  "advice": "è¯·è¯´æ˜ä½ å¯¹æ¨¡å‹å›å¤åœ¨æ‹ŸäººåŒ–ã€æ¸…æ™°åº¦ã€ç®€æ´åº¦ã€åé¢˜ç­‰æ–¹é¢çš„æ•´ä½“å»ºè®®"
}}

ç”¨æˆ·è¾“å…¥ï¼š{input_text}
å‚è€ƒå›å¤ï¼š{output}
æ¨¡å‹å›å¤ï¼š{model_output}

è¯·æŒ‰å¦‚ä¸‹ JSON æ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼ˆåŒ…å«è¯„åˆ†å’Œå»ºè®®ï¼‰ï¼š
{{
  "human_likeness": X,
  "clarity": X,
  "conciseness": X,
  "on_topic": X,
  "advice": "è¯·è¯´æ˜ä½ å¯¹æ¨¡å‹å›å¤åœ¨æ‹ŸäººåŒ–ã€æ¸…æ™°åº¦ã€ç®€æ´åº¦ã€åé¢˜ç­‰æ–¹é¢çš„æ•´ä½“å»ºè®®"
}}
""".strip()


async def evaluate_one(
    dialogue_client: OpenAILLMClient, scoring_client: OpenAILLMClient, item: dict
) -> dict:
    # ç”Ÿæˆæ¨¡å‹å›å¤
    system = {
        "role": "system",
        "content": DIALOGUE_SYSTEM_PROMPT,
        # "content": "",
    }
    prompt = {"role": "user", "content": f"ç”¨æˆ·è¯´ï¼š{item['input']}"}
    # final_prompt = DIALOGUE_USER_PROMPT.replace("{{USER_TEXT}}", item["input"])
    # prompt = {"role": "user", "content": final_prompt}
    model_response = await dialogue_client.chat(system, prompt, history=[])
    try:
        model_output = model_response.choices[0].message.content.strip()
    except Exception as e:
        return {
            "input": item["input"],
            "output": item["output"],
            "model_output": "",
            "semantic_similarity": "-",
            "score": {
                "human_likeness": "-",
                "clarity": "-",
                "conciseness": "-",
                "on_topic": "-",
                "total": "-",
            },
            "error": f"model_response error: {str(e)}",
        }
    item["model_output"] = model_output

    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
    try:
        embedder = get_aliyun_embedder()
        embeddings = await embedder.embed_texts([item["output"], model_output])
        vec1, vec2 = np.array(embeddings[0]), np.array(embeddings[1])
        cos_sim = float(
            np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        )
        semantic_similarity = round(cos_sim * 100, 2)  # æ˜ å°„åˆ° 0~100 åˆ†
    except Exception as e:
        semantic_similarity = "-"

    # æ„å»ºè¯„åˆ†æç¤ºè¯å¹¶è°ƒç”¨è¯„åˆ†æ¨¡å‹
    score_prompt = {
        "role": "user",
        "content": build_eval_prompt(item["input"], item["output"], model_output),
    }
    score_response = await scoring_client.chat(
        SCORING_SYSTEM_PROMPT, score_prompt, history=[]
    )
    try:
        content = score_response.choices[0].message.content.strip()
        scores = json.loads(content)
        advice = scores.pop("advice", "")
        # ä¿æŒè¯„åˆ†åŸæ ·ï¼ˆæ¨¡å‹ç›´æ¥è¿”å›ç™¾åˆ†åˆ¶ï¼‰
        scores = {
            k: round(v, 2) for k, v in scores.items() if isinstance(v, (int, float))
        }
        return {
            "input": item["input"],
            "output": item["output"],
            "model_output": model_output,
            "semantic_similarity": semantic_similarity,
            "score": {
                **scores,
                "total": round(sum(scores.values()) / len(scores), 2),
            },
            "advice": advice,
        }
    except Exception as e:
        return {
            "input": item["input"],
            "output": item["output"],
            "model_output": model_output,
            "semantic_similarity": semantic_similarity,
            "score": {
                "human_likeness": "-",
                "clarity": "-",
                "conciseness": "-",
                "on_topic": "-",
                "total": "-",
            },
            "error": f"score_response error: {str(e)}",
        }


async def run_prompt_eval(
    start: int = 0, end: int | None = None, round_id: int = 1
) -> list[dict]:
    input_path = Path(__file__).parent / "prompt_eval_100.jsonl"
    dialogue_client = OpenAILLMClient(LLMModelConfig.DOUBAO_1_5_LITE)
    scoring_client = OpenAILLMClient(LLMModelConfig.QWEN_PLUS_LATEST)

    results = []
    with input_path.open("r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f if line.strip()]
    data = data[start:end]
    for idx, item in enumerate(data):
        if not item.get("output"):
            continue
        result = await evaluate_one(dialogue_client, scoring_client, item)
        results.append(result)
        print(
            f"âœ… [{idx}] å®Œæˆè¯„ä¼°ï¼š{item['input'][:20]}... æ€»åˆ†ï¼š{result['score'].get('total', '-')}"
        )

    md_path = Path(__file__).parent / "prompt_eval_result.md"
    with md_path.open("w", encoding="utf-8") as f:
        f.write(f"# ğŸ“‹ è¯„ä¼°ç»“æœï¼ˆç¬¬ {round_id} è½®ï¼Œå…± {len(results)} æ¡ï¼‰\n\n")
        f.write(f"**ä½¿ç”¨æç¤ºè¯ï¼š**\n{DIALOGUE_SYSTEM_PROMPT}\n\n")
        f.write("**æ¨¡å‹æ¸©åº¦ï¼š** 0\n\n")
        f.write(f"##  ğŸ–¨ æ¨¡å‹å›å¤\n\n")
        f.write(
            "| ç”¨æˆ·è¾“å…¥ | æ¨¡å‹è¾“å‡º | å‚è€ƒè¾“å‡º | ç›¸ä¼¼åº¦ | æ‹ŸäººåŒ–ï¼ˆ0~100åˆ†ï¼‰ | æ¸…æ™°åº¦ï¼ˆ0~100åˆ†ï¼‰ | ç®€æ´åº¦ï¼ˆ0~100åˆ†ï¼‰ | åé¢˜ç¨‹åº¦ï¼ˆ0~100åˆ†ï¼‰ | æ€»åˆ† | å»ºè®® |\n"
        )
        f.write(
            "|----------|----------|----------|--------|------------------|------------------|------------------|--------------------|------|------|\n"
        )
        for item in results:
            score = item.get("score", {})
            row = "| {input} | {model_output} | {output} | {semantic_similarity} | {human_likeness} | {clarity} | {conciseness} | {on_topic} | {total} | {advice} |\n".format(
                input=item["input"].replace("\n", " "),
                model_output=item.get("model_output", "").replace("\n", " "),
                output=item.get("output", "").replace("\n", " "),
                semantic_similarity=item.get("semantic_similarity", "-"),
                human_likeness=score.get("human_likeness", "-"),
                clarity=score.get("clarity", "-"),
                conciseness=score.get("conciseness", "-"),
                on_topic=score.get("on_topic", "-"),
                total=score.get("total", "-"),
                advice=item.get("advice", "").replace("\n", " "),
            )
            f.write(row)

        # ç»Ÿè®¡ç»´åº¦å¹³å‡åˆ†
        valid_scores = [
            item["score"]
            for item in results
            if isinstance(item.get("score"), dict) and item["score"].get("total") != "-"
        ]
        if valid_scores:
            avg = lambda key: round(
                sum(
                    float(s.get(key, 0))
                    for s in valid_scores
                    if s.get(key) not in ["-", None]
                )
                / len(valid_scores),
                2,
            )
            f.write(f"\n\n## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n")
            f.write(f"- æ€»è¯„ä¼°æ•°æ®é‡ï¼š{len(results)} æ¡\n")
            f.write(f"- æœ‰æ•ˆè¯„åˆ†æ•°é‡ï¼š{len(valid_scores)} æ¡\n")
            f.write("\n\n## ğŸ“ˆ å¹³å‡è¯„åˆ†ç»Ÿè®¡\n\n")
            f.write(
                f"- ç›¸ä¼¼åº¦å¹³å‡åˆ†ï¼š{round(sum(float(item['semantic_similarity']) for item in results if isinstance(item['semantic_similarity'], (int, float))) / len(valid_scores), 2)}\n"
            )
            f.write(f"- æ‹ŸäººåŒ–å¹³å‡åˆ†ï¼š{avg('human_likeness')}\n")
            f.write(f"- æ¸…æ™°åº¦å¹³å‡åˆ†ï¼š{avg('clarity')}\n")
            f.write(f"- åé¢˜ç¨‹åº¦å¹³å‡åˆ†ï¼š{avg('on_topic')}\n")
            f.write(f"- æ€»åˆ†å¹³å‡å€¼ï¼š{avg('total')}\n")

    return results
