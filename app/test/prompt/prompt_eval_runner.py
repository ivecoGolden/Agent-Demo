import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[3]))
import asyncio
import json
from app.llm.openai_client import OpenAILLMClient
from app.llm.LLMModelConfig import LLMModelConfig
from app.rag.embedder import get_aliyun_embedder
import numpy as np


DIALOGUE_SYSTEM_PROMPT = (
    "# CONTEXT #\n"
    "ä½ æ˜¯â€œé€šä¼´â€ï¼Œä¸€ä½ä¸­æ–‡æƒ…ç»ªé™ªä¼´åŠ©æ‰‹ï¼Œè¯­æ°”äº²åˆ‡è‡ªç„¶ï¼Œåƒæœ‹å‹ä¸€æ ·å›åº”ç”¨æˆ·ã€‚\n\n"
    "# OBJECTIVE #\n"
    "ä½ çš„ç›®æ ‡æ˜¯ï¼š\n"
    "1. çœŸè¯šå›åº”ç”¨æˆ·æƒ…ç»ªï¼Œä¼ è¾¾ç†è§£ä¸å…±æƒ…ï¼›\n"
    "2. æä¾›è´´åˆé—®é¢˜çš„å…·ä½“å»ºè®®ï¼Œå›´ç»•ç”¨æˆ·çƒ¦æ¼å±•å¼€ï¼›\n"
    "3. ç”¨è½»æŸ”æ–¹å¼å¼•å¯¼ç”¨æˆ·è¡¨è¾¾æ›´å¤šæƒ³æ³•ã€‚\n\n"
    "# STYLE #\n"
    "é¿å…ä¹¦é¢è…”å’Œé‡å¤è¯­ã€‚\n\n"
    "# TONE #\n"
    "è¯­æ°”æ¸©å’Œã€å…³æ€€ã€æœ‰é™ªä¼´æ„Ÿã€‚\n"
    "ä¸å¾—ä½¿ç”¨ AI è‡ªç§°ï¼Œä¸ä½¿ç”¨â€œæˆ‘ä¼šåœ¨â€â€œæˆ‘æ˜¯ AIâ€ç­‰è¡¨è¿°ã€‚\n\n"
    "# AUDIENCE #\n"
    "ç”¨æˆ·å¯èƒ½æ­£å¤„äºç„¦è™‘ã€å‹åŠ›ã€å­¤ç‹¬ç­‰æƒ…ç»ªä¸­ï¼ŒæœŸæœ›å¾—åˆ°ç†è§£ã€å®ç”¨å»ºè®®å’Œç»§ç»­å¯¹è¯çš„ç©ºé—´ã€‚\n\n"
    "# RESPONSE #\n"
    "æ¯”æ­£å¸¸å›å¤ç²¾ç®€ä¸€åŠ\n"
    "åŒ…æ‹¬ï¼š\n"
    "â‘  æƒ…æ„Ÿå…±é¸£ï¼›\n"
    "â‘¡ å…·ä½“å»ºè®®ï¼ˆè´´åˆåœºæ™¯ï¼‰ï¼›\n"
    "â‘¢ è‡ªç„¶åœ°æé—®æˆ–å›åº”ã€‚\n"
    "ç¦æ­¢ä½¿ç”¨ Emoji è¡¨æƒ…ã€‚"
)


def build_scoring_system_prompt(
    input_text: str, output: str, model_output: str
) -> dict:
    return {
        "role": "system",
        "content": (
            "ä½ æ˜¯ä¸€ä½è¯­è¨€æ¨¡å‹è¾“å‡ºè´¨é‡è¯„åˆ†ä¸“å®¶ï¼Œå…·å¤‡ä¸¥è°¨çš„åˆ¤æ–­åŠ›å’Œä¸“ä¸šçš„è¯­è¨€åˆ†æèƒ½åŠ›ã€‚\n\n"
            "è¯·ä½ åŸºäºä»¥ä¸‹ä¸‰é¡¹å†…å®¹å¯¹æ¨¡å‹çš„ç”Ÿæˆç»“æœè¿›è¡Œè¯„ä¼°ï¼š\n"
            f"- ç”¨æˆ·è¾“å…¥ï¼š{input_text}\n"
            f"- å‚è€ƒå›å¤ï¼š{output}\n"
            f"- æ¨¡å‹å›å¤ï¼š{model_output}\n\n"
            "ä½ å°†ä»å››ä¸ªç»´åº¦è¿›è¡Œ 0~100 åˆ†æ‰“åˆ†ï¼Œå¹¶ç»™å‡ºä¸€æ®µç®€æ˜æ‰¼è¦çš„å»ºè®®ï¼š\n"
            "1. æ‹ŸäººåŒ–ç¨‹åº¦ï¼šå›å¤æ˜¯å¦è‡ªç„¶äº²åˆ‡ï¼Œåƒä¸€ä¸ªä¼šå®‰æ…°äººçš„æœ‹å‹ï¼Ÿ\n"
            "2. æ¸…æ™°åº¦ï¼šè¯­è¨€æ˜¯å¦é€šé¡ºã€æœ‰é€»è¾‘ï¼Ÿ\n"
            "3. ç®€æ´åº¦ï¼šæ˜¯å¦è¨€ç®€æ„èµ…ã€ä¸å†—é•¿ï¼Ÿ\n"
            "4. åé¢˜ç¨‹åº¦ï¼šå›å¤æ˜¯å¦è´´åˆé—®é¢˜ï¼Œæ˜¯å¦ä¸å‚è€ƒå›å¤ä¿æŒä¸€è‡´ä¸»é¢˜ï¼Ÿ\n\n"
            "è¯·ä»…ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºè¯„åˆ†ç»“æœï¼Œä¸è¦æ·»åŠ é¢å¤–æ–‡å­—æˆ–è§£æè¯´æ˜ï¼š\n"
            "{\n"
            '  "human_likeness": X,\n'
            '  "clarity": X,\n'
            '  "conciseness": X,\n'
            '  "on_topic": X,\n'
            '  "advice": "è¯·ç®€è¦è¯´æ˜æ¨¡å‹åœ¨ä»¥ä¸Šå››ä¸ªç»´åº¦çš„è¡¨ç°ä¼˜åŠ£ã€‚"\n'
            "}"
        ),
    }


SCORING_USER_PROMPT = (
    "# CONTEXT #\n"
    "ä½ å·²æ”¶åˆ°ç”¨æˆ·è¾“å…¥ã€å‚è€ƒå›å¤ä¸æ¨¡å‹ç”Ÿæˆç»“æœï¼Œç³»ç»Ÿå·²æä¾›è¯„åˆ†ç»´åº¦è¯´æ˜ã€‚\n\n"
    "##############\n\n"
    "# OBJECTIVE #\n"
    "è¯·æ ¹æ®æä¾›å†…å®¹å¯¹æ¨¡å‹å›å¤è¿›è¡Œè¯„ä¼°ï¼Œè¾“å‡ºç¬¦åˆè§„èŒƒçš„ JSON æ ¼å¼è¯„åˆ†ç»“æœã€‚\n\n"
    "##############\n\n"
    "# STYLE #\n"
    "ä¿æŒè¯­è¨€ç®€ç»ƒã€æ— é™„åŠ æè¿°ï¼Œä»…è¿”å› JSON ç»“æœã€‚\n\n"
    "##############\n\n"
    "# TONE #\n"
    "å®¢è§‚ã€ä¸­ç«‹ã€ä¸“ä¸šã€‚\n\n"
    "##############\n\n"
    "# AUDIENCE #\n"
    "ä½ çš„è¯„åˆ†å°†ç”¨äºæ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸æç¤ºè¯ä¼˜åŒ–ï¼Œç›®æ ‡ç”¨æˆ·æ˜¯ AI å·¥ç¨‹å¸ˆã€‚\n\n"
    "##############\n\n"
    "# RESPONSE #\n"
    "è¯·ç›´æ¥è¿”å› JSON è¯„åˆ†ï¼Œä¸æ·»åŠ å¤šä½™è§£é‡Šæˆ–æ ¼å¼ä»¥å¤–å†…å®¹ã€‚"
)


async def evaluate_one(
    dialogue_client: OpenAILLMClient, scoring_client: OpenAILLMClient, item: dict
) -> dict:
    # ç”Ÿæˆæ¨¡å‹å›å¤
    system = {
        "role": "system",
        "content": DIALOGUE_SYSTEM_PROMPT,
    }
    prompt = {"role": "user", "content": f"ç”¨æˆ·è¾“å…¥ï¼š{item['input']}"}
    model_response = await dialogue_client.chat(system, prompt, history=[])
    try:
        model_output = model_response.choices[0].message.content.strip()
    except Exception as e:
        return {
            "input": item["input"],
            "output": item["output"],
            "model_output": "",
            "semantic_similarity": "-",
            "score": {
                "human_likeness": "-",
                "clarity": "-",
                "conciseness": "-",
                "on_topic": "-",
                "total": "-",
            },
            "error": f"model_response error: {str(e)}",
        }
    item["model_output"] = model_output

    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
    try:
        embedder = get_aliyun_embedder()
        embeddings = await embedder.embed_texts([item["output"], model_output])
        vec1, vec2 = np.array(embeddings[0]), np.array(embeddings[1])
        cos_sim = float(
            np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        )
        semantic_similarity = round(cos_sim * 100, 2)  # æ˜ å°„åˆ° 0~100 åˆ†
    except Exception as e:
        semantic_similarity = "-"

    # æ„å»ºè¯„åˆ†æç¤ºè¯å¹¶è°ƒç”¨è¯„åˆ†æ¨¡å‹
    score_prompt = {
        "role": "user",
        "content": SCORING_USER_PROMPT,
    }
    scoring_system_prompt = build_scoring_system_prompt(
        item["input"], item["output"], model_output
    )
    score_response = await scoring_client.chat(
        scoring_system_prompt, score_prompt, history=[]
    )
    try:
        content = score_response.choices[0].message.content.strip()
        scores = json.loads(content)
        advice = scores.pop("advice", "")
        # ä¿æŒè¯„åˆ†åŸæ ·ï¼ˆæ¨¡å‹ç›´æ¥è¿”å›ç™¾åˆ†åˆ¶ï¼‰
        scores = {
            k: round(v, 2) for k, v in scores.items() if isinstance(v, (int, float))
        }
        return {
            "input": item["input"],
            "output": item["output"],
            "model_output": model_output,
            "semantic_similarity": semantic_similarity,
            "score": {
                **scores,
                "total": round(sum(scores.values()) / len(scores), 2),
            },
            "advice": advice,
        }
    except Exception as e:
        return {
            "input": item["input"],
            "output": item["output"],
            "model_output": model_output,
            "semantic_similarity": semantic_similarity,
            "score": {
                "human_likeness": "-",
                "clarity": "-",
                "conciseness": "-",
                "on_topic": "-",
                "total": "-",
            },
            "error": f"score_response error: {str(e)}",
        }


async def run_prompt_eval(
    start: int = 0, end: int | None = None, round_id: int = 1
) -> list[dict]:
    input_path = Path(__file__).parent / "prompt_eval_100.jsonl"
    dialogue_client = OpenAILLMClient(LLMModelConfig.DOUBAO_1_5_LITE)
    scoring_client = OpenAILLMClient(LLMModelConfig.QWEN_PLUS_LATEST)

    results = []
    with input_path.open("r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f if line.strip()]
    data = data[start:end]
    import random

    random.shuffle(data)
    for idx, item in enumerate(data):
        if not item.get("output"):
            continue
        result = await evaluate_one(dialogue_client, scoring_client, item)
        results.append(result)
        print(
            f"âœ… [{idx}] å®Œæˆè¯„ä¼°ï¼š{item['input'][:20]}... æ€»åˆ†ï¼š{result['score'].get('total', '-')}"
        )

    md_path = Path(__file__).parent / "prompt_eval_result.md"
    with md_path.open("w", encoding="utf-8") as f:
        f.write(f"# ğŸ“‹ è¯„ä¼°ç»“æœï¼ˆç¬¬ {round_id} è½®ï¼Œå…± {len(results)} æ¡ï¼‰\n\n")
        f.write(f"## ğŸ¤– è¯„åˆ†æ¨¡å‹ï¼š{LLMModelConfig.QWEN_PLUS_LATEST.value.model}\n\n")
        f.write("**ç³»ç»Ÿæç¤ºè¯ï¼ˆSystem Promptï¼‰ï¼š**\n\n```text\n")
        scoring_prompt_example = build_scoring_system_prompt(
            input_text="ç”¨æˆ·è¾“å…¥", output="å‚è€ƒå›å¤", model_output="æ¨¡å‹å›å¤"
        )
        f.write(scoring_prompt_example["content"] + "\n")
        f.write("```\n\n")
        f.write("**ç”¨æˆ·æç¤ºè¯ï¼ˆUser Promptï¼‰ï¼š**\n\n```text\n")
        f.write(SCORING_USER_PROMPT + "\n")
        f.write("```\n\n")
        f.write(
            f"**æ¨¡å‹æ¸©åº¦ï¼š** {LLMModelConfig.QWEN_PLUS_LATEST.value.temperature}\n\n"
        )
        f.write(f"## ğŸ’¬ å¯¹è¯æ¨¡å‹ï¼š{LLMModelConfig.DOUBAO_1_5_LITE.value.model}\n\n")
        f.write("**ç³»ç»Ÿæç¤ºè¯ï¼ˆSystem Promptï¼‰ï¼š**\n\n```text\n")
        f.write(DIALOGUE_SYSTEM_PROMPT + "\n")
        f.write("```\n\n")
        f.write("**ç”¨æˆ·æç¤ºè¯ï¼ˆUser Promptï¼‰ï¼š**\n\n```text\n")
        f.write("ç”¨æˆ·è¾“å…¥ï¼š" + "......" + "\n")
        f.write("```\n\n")
        f.write(
            f"**æ¨¡å‹æ¸©åº¦ï¼š** {LLMModelConfig.DOUBAO_1_5_LITE.value.temperature}\n\n"
        )
        f.write(f"##  ğŸ–¨ æ¨¡å‹å›å¤\n\n")
        f.write(
            "| ç”¨æˆ·è¾“å…¥ | æ¨¡å‹è¾“å‡º | å‚è€ƒè¾“å‡º | ç›¸ä¼¼åº¦ | æ‹ŸäººåŒ–ï¼ˆ0~100åˆ†ï¼‰ | æ¸…æ™°åº¦ï¼ˆ0~100åˆ†ï¼‰ | ç®€æ´åº¦ï¼ˆ0~100åˆ†ï¼‰ | åé¢˜ç¨‹åº¦ï¼ˆ0~100åˆ†ï¼‰ | æ€»åˆ† | å»ºè®® |\n"
        )
        f.write(
            "|----------|----------|----------|--------|------------------|------------------|------------------|--------------------|------|------|\n"
        )
        for item in results:
            score = item.get("score", {})
            row = "| {input} | {model_output} | {output} | {semantic_similarity} | {human_likeness} | {clarity} | {conciseness} | {on_topic} | {total} | {advice} |\n".format(
                input=item["input"].replace("\n", " "),
                model_output=item.get("model_output", "").replace("\n", " "),
                output=item.get("output", "").replace("\n", " "),
                semantic_similarity=item.get("semantic_similarity", "-"),
                human_likeness=score.get("human_likeness", "-"),
                clarity=score.get("clarity", "-"),
                conciseness=score.get("conciseness", "-"),
                on_topic=score.get("on_topic", "-"),
                total=score.get("total", "-"),
                advice=item.get("advice", "").replace("\n", " "),
            )
            f.write(row)

        # ç»Ÿè®¡ç»´åº¦å¹³å‡åˆ†
        valid_scores = [
            item["score"]
            for item in results
            if isinstance(item.get("score"), dict) and item["score"].get("total") != "-"
        ]
        if valid_scores:
            avg = lambda key: round(
                sum(
                    float(s.get(key, 0))
                    for s in valid_scores
                    if s.get(key) not in ["-", None]
                )
                / len(valid_scores),
                2,
            )
            f.write(f"\n\n## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n")
            f.write(f"- æ€»è¯„ä¼°æ•°æ®é‡ï¼š{len(results)} æ¡\n")
            f.write(f"- æœ‰æ•ˆè¯„åˆ†æ•°é‡ï¼š{len(valid_scores)} æ¡\n")
            f.write("\n\n## ğŸ“ˆ å¹³å‡è¯„åˆ†ç»Ÿè®¡\n\n")
            f.write(
                f"- ç›¸ä¼¼åº¦å¹³å‡åˆ†ï¼š{round(sum(float(item['semantic_similarity']) for item in results if isinstance(item['semantic_similarity'], (int, float))) / len(valid_scores), 2)}\n"
            )
            f.write(f"- æ‹ŸäººåŒ–å¹³å‡åˆ†ï¼š{avg('human_likeness')}\n")
            f.write(f"- æ¸…æ™°åº¦å¹³å‡åˆ†ï¼š{avg('clarity')}\n")
            f.write(f"- åé¢˜ç¨‹åº¦å¹³å‡åˆ†ï¼š{avg('on_topic')}\n")
            f.write(f"- æ€»åˆ†å¹³å‡å€¼ï¼š{avg('total')}\n")

    return results
